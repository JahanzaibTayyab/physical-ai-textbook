---
sidebar_position: 1
title: "Introduction to Vision-Language-Action"
description: "Integrating LLMs with robotics for natural human-robot interaction"
---

import PersonalizationButton from '@site/src/components/PersonalizationButton';
import TranslationButton from '@site/src/components/TranslationButton';

<PersonalizationButton 
  chapterPath="/docs/module-4-vla/intro"
  originalContent="sidebar_position: 1"
/>

<TranslationButton 
  chapterPath="/docs/module-4-vla/intro"
  originalContent="sidebar_position: 1"
/>

# Introduction to Vision-Language-Action

Welcome to Module 4: Vision-Language-Action (VLA). This module brings together everything you've learned to create robots that understand natural language, perceive their environment, and take appropriate actions.

## What is VLA?

Vision-Language-Action combines:
- **Vision**: Camera and sensor perception
- **Language**: Natural language understanding via LLMs
- **Action**: Robot control and manipulation

This enables natural human-robot interaction where humans can give commands in plain language.

## Module Learning Outcomes

By the end of this module, you will be able to:

1. **Integrate Whisper** for voice command recognition
2. **Use LLMs** to translate natural language to robot actions
3. **Implement multi-modal** interaction (speech, gesture, vision)
4. **Build a complete** autonomous humanoid system
5. **Deploy a capstone project** demonstrating all capabilities

## Module Structure

1. **Voice-to-Action** - OpenAI Whisper integration
2. **Cognitive Planning** - LLM-based task planning
3. **Multi-Modal Interaction** - Combining modalities
4. **Capstone Project** - Complete autonomous system

## Prerequisites

- Completed Modules 1, 2, and 3
- Understanding of ROS 2, simulation, and AI
- Basic knowledge of LLMs and NLP

[Next: Voice-to-Action â†’](./voice-to-action.md)